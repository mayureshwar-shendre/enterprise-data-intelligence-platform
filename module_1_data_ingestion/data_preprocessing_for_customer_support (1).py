# -*- coding: utf-8 -*-
"""Data_Preprocessing_for_Customer_Support.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18VwgExLlRd52N7NblwEyPtovGH4bZQRQ

#AI Knowledge Graph Builder for Enterprise Intelligence
##Module 1: Data Injection & Preprocessing

1.   Data Cleaning
2.   Data Validation
1.   Data Transformation
2.   Data Filtering
1.   Data Enrichment
2.   Data Deduplication
1.   Data Masking & Security
2.   Data Standardization
1.   Error Handling & Logging
2.   Metadata Handling
1.   Sampling (Optional)

##Data Preprocessing for Customer Support Dataset
"""

'''
Install required libraries:
    1. Pandas
    2. Numpy
    3. Scikit-Learn
    4. nltk
    5. Matplotlib
    6. Seaborn
'''
!pip install pandas numpy scikit-learn nltk matplotlib seaborn

# Import after installation
import pandas as pd
import numpy as np

# Upload file(.csv) in Colab from the Computer
#from google.colab import files
#uploaded = files.upload()
# Select the CSV file from your computer

# Upload the file manually
df = pd.read_csv('/content/customer_support_tickets.csv')

#Showing the data in .csv file
df

"""### METHOD 1: DATA CLEANING
####Purpose: Remove bad data, fix missing values, remove duplicates

STEP 1: Perform & Display Basic Operation
"""

# To check the Number of rows and Column present in the Data sheet by using 'shape function'
df.shape

#OR
print(f"Total rows: {len(df)}")
print(f"Total columns: {len(df.columns)}")

# View the first 5 entries from the datasets
print("Five Entries from Beginning: ")
print("=" * 80)

df.head(5)

# View the last 5 entries from the datasets
print("Five Entries from Ending:")
print("=" * 80)

df.tail(5)

# Information regarding datasets (Including Rows, Columns, Datatypes, Counts of sections)
print("Basic Information: ")
print("=" * 80)

df.info()
print()

#Print Datatypes
print(f"\nData types:\n{df.dtypes}")

# Counting number of same values under each catergoies i.e Gender category (Male & females)
print(df['Customer Gender'].value_counts())      #Data for gender
print()
print("=" * 80)

print(df['Ticket Type'].value_counts())          #Data for Type of ticket
print()
print("=" * 80)

print(df['Ticket Status'].value_counts())        #Data for Status of ticket
print()
print("=" * 80)

print(df['Ticket Channel'].value_counts())       #Data for Channel of ticket
print()
print("=" * 80)

"""STEP 2: Handle Missing Values"""

# To count the null or missingh values in the datasets under each COLUMN
print("Missing values count from each Column:")
print("=" * 80)

df.isna().sum()

# To check any null value(missing value) present in datasets
# It shows boolean values as: true(as null value present) and false(as no null value is present)
df.isna()

# Fill text columns with "Unknown"
text_columns = df.select_dtypes(include=['object']).columns
for col in text_columns:
    if df[col].isnull().sum() > 0:
        df[col] = df[col].fillna('Unknown')
        print(f"✓ Filled '{col}' with 'Unknown'")

# Fill numeric columns with median
numeric_columns = df.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    if df[col].isnull().sum() > 0:
        median_val = df[col].median()
        df[col] = df[col].fillna(median_val)
        print(f"✓ Filled '{col}' with median: {median_val}")

df

"""STEP 3: Remove Duplicates"""

#Checking for duplicates values in the datasets
print("REMOVING DUPLICATE RECORDS:")
print("=" * 80)

duplicate_count = df.duplicated().sum()
print(f"Found {duplicate_count} duplicate rows")

df = df.drop_duplicates()
print(f"✓ Removed duplicates. New size: {len(df)} rows")

"""STEP 4: Standardize Column Names"""

# Rename the Column Names in Standard formats
print("STANDARDIZING COLUMN NAMES:")
print("=" * 80)

df.columns = df.columns.str.lower().str.replace(' ', '_')
print(f"✓ Standardized names:\n{list(df.columns)}")
print("=" * 80)

df

"""STEP 5: Remove Invalid Values"""

# Removing Invalid Values
print("REMOVING INVALID VALUES: ")
print("=" * 80)

if 'customer_age' in df.columns:
    initial = len(df)
    df = df[df['customer_age'] >= 18]
    print(f"✓ Removed {initial - len(df)} rows with age < 18")

df = df.dropna(how='all')
print(f"✓ Removed completely empty rows")

"""#### Saving the Cleaned Data after applying the Data Cleaning Method"""

# Concluded:
print("DATA CLEANING COMPLETE")
print(f"Final size: {len(df)} rows × {len(df.columns)} columns")
print("=" * 80)

# Save to CSV in Google Drive
df.to_csv('01_cleaned_data.csv', index=False)
print("✓ Saved: 01_cleaned_data.csv")

"""### METHOD 2: DATA VALIDATION

STEP 1: Check Data Types
"""

# Upload Load cleaned data
df = pd.read_csv('01_cleaned_data.csv')

# Check Data Types in the cleaned data sheet
print("VALIDATING DATA TYPES:")
print("=" * 80)

print("\nCurrent data types:")
print(df.dtypes)

"""STEP 2: Range Checks"""

# Checking the range of the age group of Customers

print("CHECKING VALUE RANGES:")
print("=" * 80)

if 'customer_age' in df.columns:
    min_age = df['customer_age'].min()
    max_age = df['customer_age'].max()
    valid_age = ((df['customer_age'] >= 18) & (df['customer_age'] <= 100)).all()
    status = "✓ VALID" if valid_age else "✗ INVALID"
    print(f"{status}: Age range [{min_age}, {max_age}] (expected 18-100)")

"""STEP 3: Mandatory Field Checks"""

# Checking the Mandatory Field
print("CHECKING MANDATORY FIELDS")
print("=" * 80)

mandatory_fields = ['customer_age', 'ticket_status', 'ticket_priority']
for field in mandatory_fields:
    if field in df.columns:
        empty = df[field].isnull().sum()
        status = "✓ VALID" if empty == 0 else "✗ INVALID"
        print(f"{status}: '{field}' - {empty} empty values")

"""STEP 4: Unique ID Check"""

# Checking for the Unique ID
print("CHECKING UNIQUE IDENTIFIERS:")
print("=" * 80)

if 'ticket_id' in df.columns:
    total = len(df)
    unique = df['ticket_id'].nunique()
    is_unique = (total == unique)
    status = "✓ VALID" if is_unique else "✗ INVALID"
    print(f"{status}: {unique} unique out of {total} records")

"""STEP 5: Statistical Summary"""

# Summaries/Describing the full datasets into the various factors like Max. Min. Values, Standard values etc.
print("STATISTICAL SUMMARY:")
print("=" * 80)

print("\nNumeric columns summary:")
print(df.describe())

"""####Saving the Validated Data after applying the Data Validation Method"""

# Concluded:
print("VALIDATION COMPLETE:")
print(f"Final size: {len(df)} rows × {len(df.columns)} columns")
print("=" * 80)

# Save to CSV in Google Drive
df.to_csv('02_validated_data.csv', index=False)
print("✓ Saved: 02_validated_data.csv")

"""###METHOD 3: DATA TRANSFORMATION

STEP 1: Data Type Conversion
"""

from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from datetime import datetime

# Upload Load validated data
df = pd.read_csv('02_validated_data.csv')

# Check Data Types in the validated data sheet
print("\nCurrent data types:")
print(df.dtypes)
print("=" * 80)

# Converting the Datatypes
print("CONVERTING DATA TYPES:")

if 'ticket_status' in df.columns:
    df['ticket_status'] = df['ticket_status'].astype('category')
    print(f"✓ Converted ticket_status to category")

if 'customer_age' in df.columns:
    df['customer_age'] = df['customer_age'].astype('int32')
    print(f"✓ Converted customer_age to int32")

print("=" * 80)
# Data Types after changing
print("\nCurrent data types:")
print(df.dtypes)

"""STEP 2: DateTime Parsing"""

# DateTime Parsing :
# converting a string of text that represents a date and/or time into a structured, machine-readable format
print("PARSING DATE COLUMNS:")
print("=" * 80)

date_columns = ['date_of_purchase', 'first_response_time', 'time_to_resolution']
for col in date_columns:
    if col in df.columns and df[col].dtype == 'object':
        df[col] = pd.to_datetime(df[col], errors='coerce')
        print(f"✓ Converted '{col}' to datetime")

df

"""STEP 3: CREATING NEW FEATURES"""

print("CREATING NEW FEATURES")
print("=" * 80)

if 'date_of_purchase' in df.columns:
    df['purchase_year'] = df['date_of_purchase'].dt.year
    df['purchase_month'] = df['date_of_purchase'].dt.month
    df['purchase_day'] = df['date_of_purchase'].dt.day
    print(f"✓ Extracted year, month, day from date_of_purchase")

df

"""STEP 4: Encoding Categorical Data"""

print("ENCODING CATEGORICAL COLUMNS")
print("=" * 80)

categorical_cols = df.select_dtypes(include=['object']).columns
encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))
    encoders[col] = le
    print(f"✓ Encoded '{col}'")

df

"""STEP 5: Normalization"""

print("NORMALIZING NUMERIC COLUMNS:")
print("=" * 80)

if 'customer_age' in df.columns:
    scaler = MinMaxScaler()
    df['customer_age_normalized'] = scaler.fit_transform(df[['customer_age']])
    print(f"✓ Normalized customer_age to 0-1 range")
    print(f"  Range: [{df['customer_age_normalized'].min():.2f}, {df['customer_age_normalized'].max():.2f}]")

df

"""####Saving the Transformed Data after applying the Data Transformation Method"""

# Concluded:
print("TRANSFORMATION COMPLETE")
print(f"Final size: {len(df)} rows × {len(df.columns)} columns")
print("=" * 80)

# Save to CSV in Google Drive
df.to_csv('03_transformed_data.csv', index=False)
print("✓ Saved: 03_transformed_data.csv")

"""### METHOD 4: DATA FILTERING

STEP 1: Remove Unnecessary Columns
"""

# Upload Load Transformed data
df = pd.read_csv('03_transformed_data.csv')

# Check Data Types in the validated data sheet
print("\nCurrent data types:")
print(df.dtypes)
print("=" * 80)

#Remove Unnecessary Columns
print("STEP 1: REMOVING UNNECESSARY COLUMNS")
print("=" * 80)

columns_to_remove = ['customer_name', 'customer_email']
for col in columns_to_remove:
    if col in df.columns:
        df = df.drop(columns=[col])
        print(f"✓ Removed: '{col}'")

# Remove original text columns that we encoded
text_cols = df.select_dtypes(include=['object']).columns
for col in text_cols:
    if col + '_encoded' in df.columns:
        df = df.drop(columns=[col])
        print(f"✓ Removed: '{col}' (using encoded version)")

df

"""STEP 2: Filter Records by Condition"""

# Filter Records by applying Condition
print("FILTERING RECORDS:")
print("=" * 80)

if 'customer_satisfaction_rating' in df.columns:
    df = df.dropna(subset=['customer_satisfaction_rating'])
    print(f"✓ Filtered out unresolved tickets")

if 'purchase_year' in df.columns:
    recent_year = df['purchase_year'].max() - 2
    df = df[df['purchase_year'] >= recent_year]
    print(f"✓ Kept only recent data (year >= {recent_year})")

df

"""STEP 3: Remove Incomplete Records"""

# Remove Incomplete Records form the datasets
print("FILTERING INCOMPLETE RECORDS:")
print("=" * 80)

initial = len(df)
critical_fields = ['customer_age', 'ticket_status', 'ticket_priority']
df = df.dropna(subset=[col for col in critical_fields if col in df.columns])
print(f"✓ Removed {initial - len(df)} rows with missing critical fields")

print(f"\nAfter: {len(df)} rows × {len(df.columns)} columns")

"""####Saving the Filtered Data after applying the Data Filtration Method"""

# Concluded:
print("DATA FILTERING COMPLETE")
print(f"Final size: {len(df)} rows × {len(df.columns)} columns")
print("=" * 80)

# Save to CSV in Google Drive
df.to_csv('04_filtered_data.csv', index=False)
print("✓ Saved: 04_filtered_data.csv")

"""###METHOD 5: DATA ENRICHMENT

STEP 1: Create Derived Fields
"""

# Upload Load Filtered data
df = pd.read_csv('04_filtered_data.csv')

# Create Derived Fields
print("CREATING DERIVED FIELDS:")
print("=" * 80)

def categorize_age(age):
    """Categorize age into groups"""
    if age < 30:
        return 'Young'
    elif age < 50:
        return 'Middle-aged'
    else:
        return 'Senior'

if 'customer_age' in df.columns:
    df['age_group'] = df['customer_age'].apply(categorize_age)
    print("✓ Created age_group field")
    print(f"  Categories: {df['age_group'].unique()}")

df

#Visual Representation of the data
import seaborn as sns
import matplotlib.pyplot as plt

# 1. Set the visual style
sns.set_theme(style="whitegrid")
plt.figure(figsize=(10, 6))

# 2. Define the desired order for the categories
age_order = ['Young', 'Middle-aged', 'Senior']

# 3. Create the countplot
ax = sns.countplot(
    data=df,
    x='age_group',
    order=age_order,
    palette='viridis'
)

# 4. Add labels and title for clarity
plt.title('Distribution of Customers by Age Group', fontsize=15, pad=20)
plt.xlabel('Age Group', fontsize=12)
plt.ylabel('Number of Customers', fontsize=12)

# 5. Add data labels on top of each bar (optional but helpful)
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}',
                (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center',
                xytext=(0, 9),
                textcoords='offset points')

plt.show()

"""STEP 2: Lookup-Based Enrichment"""

print("ADDING REFERENCE DATA:")
print("=" * 80)

priority_mapping = {
    'low': 1,
    'medium': 2,
    'high': 3,
    'critical': 4
}

print("✓ Created priority mapping reference")

"""STEP 4: Interaction Features"""

print("CREATING INTERACTION FEATURES:")
print("=" * 80)

if 'customer_age' in df.columns and 'customer_satisfaction_rating' in df.columns:
    df['customer_value_score'] = (df['customer_age'] * df['customer_satisfaction_rating']) / 100
    print("✓ Created customer_value_score")

"""STEP 5: Time-Based Features"""

print("EXTRACTING TIME FEATURES:")
print("=" * 80)

if 'purchase_month' in df.columns:
    def get_season(month):
        if month in [12, 1, 2]:
            return 'Winter'
        elif month in [3, 4, 5]:
            return 'Spring'
        elif month in [6, 7, 8]:
            return 'Summer'
        else:
            return 'Fall'

    df['season'] = df['purchase_month'].apply(get_season)
    print("✓ Created season field")

df

"""####Saving the Enriched Data after applying the Data Enrichment Method"""

# Concluded:
print("DATA ENRICHMENT COMPLETE")
print(f"Final size: {len(df)} rows × {len(df.columns)} columns")
print("=" * 80)

# Save to CSV in Google Drive
df.to_csv('05_enriched_data.csv', index=False)
print("✓ Saved: 05_enriched_data.csv")

df