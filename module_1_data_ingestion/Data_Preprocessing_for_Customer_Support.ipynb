{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#AI Knowledge Graph Builder for Enterprise Intelligence\n",
        "##Module 1: Data Injection & Preprocessing\n",
        "\n",
        "1.   Data Cleaning\n",
        "2.   Data Validation\n",
        "1.   Data Transformation\n",
        "2.   Data Filtering\n",
        "1.   Data Enrichment\n",
        "2.   Data Deduplication\n",
        "1.   Data Masking & Security\n",
        "2.   Data Standardization\n",
        "1.   Error Handling & Logging\n",
        "2.   Metadata Handling\n",
        "1.   Sampling (Optional)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NqEhs1POb_2R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Preprocessing for Customer Support Dataset"
      ],
      "metadata": {
        "id": "BPL0MpqjhKPm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZtvJygU4P_0"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Install required libraries:\n",
        "    1. Pandas\n",
        "    2. Numpy\n",
        "    3. Scikit-Learn\n",
        "    4. nltk\n",
        "    5. Matplotlib\n",
        "    6. Seaborn\n",
        "'''\n",
        "!pip install pandas numpy scikit-learn nltk matplotlib seaborn\n",
        "\n",
        "# Import after installation\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload file(.csv) in Colab from the Computer\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "# Select the CSV file from your computer\n",
        "\n",
        "# Upload the file manually\n",
        "df = pd.read_csv('/content/customer_support_tickets.csv')\n",
        "\n",
        "#Showing the data in .csv file\n",
        "df"
      ],
      "metadata": {
        "id": "orQVDFvsoM6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METHOD 1: DATA CLEANING\n",
        "####Purpose: Remove bad data, fix missing values, remove duplicates"
      ],
      "metadata": {
        "id": "M31uSfPs7-iK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Perform & Display Basic Operation"
      ],
      "metadata": {
        "id": "_YUvsnkx8nr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To check the Number of rows and Column present in the Data sheet by using 'shape function'\n",
        "df.shape\n",
        "\n",
        "#OR\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")"
      ],
      "metadata": {
        "id": "_R9tIwlRBDP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the first 5 entries from the datasets\n",
        "print(\"Five Entries from Beginning: \")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "z9T2VSrRrqT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the last 5 entries from the datasets\n",
        "print(\"Five Entries from Ending:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df.tail(5)"
      ],
      "metadata": {
        "id": "pv8YTdEqsCPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Information regarding datasets (Including Rows, Columns, Datatypes, Counts of sections)\n",
        "print(\"Basic Information: \")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df.info()\n",
        "print()\n",
        "\n",
        "#Print Datatypes\n",
        "print(f\"\\nData types:\\n{df.dtypes}\")"
      ],
      "metadata": {
        "id": "dsJWjfhksyzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting number of same values under each catergoies i.e Gender category (Male & females)\n",
        "print(df['Customer Gender'].value_counts())      #Data for gender\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(df['Ticket Type'].value_counts())          #Data for Type of ticket\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(df['Ticket Status'].value_counts())        #Data for Status of ticket\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(df['Ticket Channel'].value_counts())       #Data for Channel of ticket\n",
        "print()\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "8_WPIYLZzWSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Handle Missing Values"
      ],
      "metadata": {
        "id": "qpECg4WuB5O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To count the null or missingh values in the datasets under each COLUMN\n",
        "print(\"Missing values count from each Column:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df.isna().sum()\n"
      ],
      "metadata": {
        "id": "BIVKp0I6tj9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To check any null value(missing value) present in datasets\n",
        "# It shows boolean values as: true(as null value present) and false(as no null value is present)\n",
        "df.isna()"
      ],
      "metadata": {
        "id": "KTQ-FJC1uucX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill text columns with \"Unknown\"\n",
        "text_columns = df.select_dtypes(include=['object']).columns\n",
        "for col in text_columns:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        df[col] = df[col].fillna('Unknown')\n",
        "        print(f\"✓ Filled '{col}' with 'Unknown'\")\n",
        "\n",
        "# Fill numeric columns with median\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_columns:\n",
        "    if df[col].isnull().sum() > 0:\n",
        "        median_val = df[col].median()\n",
        "        df[col] = df[col].fillna(median_val)\n",
        "        print(f\"✓ Filled '{col}' with median: {median_val}\")"
      ],
      "metadata": {
        "id": "PpZ_QMeGIGSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "d7ieUAjvSum-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: Remove Duplicates"
      ],
      "metadata": {
        "id": "iSWRIyqjSzyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for duplicates values in the datasets\n",
        "print(\"REMOVING DUPLICATE RECORDS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "duplicate_count = df.duplicated().sum()\n",
        "print(f\"Found {duplicate_count} duplicate rows\")\n",
        "\n",
        "df = df.drop_duplicates()\n",
        "print(f\"✓ Removed duplicates. New size: {len(df)} rows\")\n"
      ],
      "metadata": {
        "id": "GpHEILRySyqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: Standardize Column Names"
      ],
      "metadata": {
        "id": "qaXCfKdIUBK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename the Column Names in Standard formats\n",
        "print(\"STANDARDIZING COLUMN NAMES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df.columns = df.columns.str.lower().str.replace(' ', '_')\n",
        "print(f\"✓ Standardized names:\\n{list(df.columns)}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "rjYHzB3eUFPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Remove Invalid Values"
      ],
      "metadata": {
        "id": "HiclyE7ZV0UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Invalid Values\n",
        "print(\"REMOVING INVALID VALUES: \")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'customer_age' in df.columns:\n",
        "    initial = len(df)\n",
        "    df = df[df['customer_age'] >= 18]\n",
        "    print(f\"✓ Removed {initial - len(df)} rows with age < 18\")\n",
        "\n",
        "df = df.dropna(how='all')\n",
        "print(f\"✓ Removed completely empty rows\")"
      ],
      "metadata": {
        "id": "iR2QvOsBVzEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the Cleaned Data after applying the Data Cleaning Method"
      ],
      "metadata": {
        "id": "JDn_yH6SXTWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concluded:\n",
        "print(\"DATA CLEANING COMPLETE\")\n",
        "print(f\"Final size: {len(df)} rows × {len(df.columns)} columns\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('01_cleaned_data.csv', index=False)\n",
        "print(\"✓ Saved: 01_cleaned_data.csv\")\n"
      ],
      "metadata": {
        "id": "uI5uQYM65rhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METHOD 2: DATA VALIDATION"
      ],
      "metadata": {
        "id": "TFNJUZJ0ZZYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Check Data Types"
      ],
      "metadata": {
        "id": "EF57l56prRcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload Load cleaned data\n",
        "df = pd.read_csv('01_cleaned_data.csv')\n",
        "\n",
        "# Check Data Types in the cleaned data sheet\n",
        "print(\"VALIDATING DATA TYPES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nCurrent data types:\")\n",
        "print(df.dtypes)"
      ],
      "metadata": {
        "id": "GcT9S8RVspHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Range Checks"
      ],
      "metadata": {
        "id": "N0kGUdW-s_e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the range of the age group of Customers\n",
        "\n",
        "print(\"CHECKING VALUE RANGES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'customer_age' in df.columns:\n",
        "    min_age = df['customer_age'].min()\n",
        "    max_age = df['customer_age'].max()\n",
        "    valid_age = ((df['customer_age'] >= 18) & (df['customer_age'] <= 100)).all()\n",
        "    status = \"✓ VALID\" if valid_age else \"✗ INVALID\"\n",
        "    print(f\"{status}: Age range [{min_age}, {max_age}] (expected 18-100)\")\n"
      ],
      "metadata": {
        "id": "kluKeh2JswbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: Mandatory Field Checks"
      ],
      "metadata": {
        "id": "u18mThYetslP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the Mandatory Field\n",
        "print(\"CHECKING MANDATORY FIELDS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "mandatory_fields = ['customer_age', 'ticket_status', 'ticket_priority']\n",
        "for field in mandatory_fields:\n",
        "    if field in df.columns:\n",
        "        empty = df[field].isnull().sum()\n",
        "        status = \"✓ VALID\" if empty == 0 else \"✗ INVALID\"\n",
        "        print(f\"{status}: '{field}' - {empty} empty values\")"
      ],
      "metadata": {
        "id": "TatoP_UntkcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: Unique ID Check"
      ],
      "metadata": {
        "id": "I69TWQYhvUki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the Unique ID\n",
        "print(\"CHECKING UNIQUE IDENTIFIERS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'ticket_id' in df.columns:\n",
        "    total = len(df)\n",
        "    unique = df['ticket_id'].nunique()\n",
        "    is_unique = (total == unique)\n",
        "    status = \"✓ VALID\" if is_unique else \"✗ INVALID\"\n",
        "    print(f\"{status}: {unique} unique out of {total} records\")"
      ],
      "metadata": {
        "id": "5o5VgAE4ut5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Statistical Summary"
      ],
      "metadata": {
        "id": "vHBD7fjBwMA5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Summaries/Describing the full datasets into the various factors like Max. Min. Values, Standard values etc.\n",
        "print(\"STATISTICAL SUMMARY:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nNumeric columns summary:\")\n",
        "print(df.describe())\n"
      ],
      "metadata": {
        "id": "V9pWPRESvlGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Saving the Validated Data after applying the Data Validation Method"
      ],
      "metadata": {
        "id": "E9PIgNfyxEjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concluded:\n",
        "print(\"VALIDATION COMPLETE:\")\n",
        "print(f\"Final size: {len(df)} rows × {len(df.columns)} columns\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('02_validated_data.csv', index=False)\n",
        "print(\"✓ Saved: 02_validated_data.csv\")"
      ],
      "metadata": {
        "id": "frb_1qciw7pg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###METHOD 3: DATA TRANSFORMATION"
      ],
      "metadata": {
        "id": "aHC-ZfRfvkqz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Data Type Conversion"
      ],
      "metadata": {
        "id": "REVD3gYrzHm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from datetime import datetime\n",
        "\n",
        "# Upload Load validated data\n",
        "df = pd.read_csv('02_validated_data.csv')\n",
        "\n",
        "# Check Data Types in the validated data sheet\n",
        "print(\"\\nCurrent data types:\")\n",
        "print(df.dtypes)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Converting the Datatypes\n",
        "print(\"CONVERTING DATA TYPES:\")\n",
        "\n",
        "if 'ticket_status' in df.columns:\n",
        "    df['ticket_status'] = df['ticket_status'].astype('category')\n",
        "    print(f\"✓ Converted ticket_status to category\")\n",
        "\n",
        "if 'customer_age' in df.columns:\n",
        "    df['customer_age'] = df['customer_age'].astype('int32')\n",
        "    print(f\"✓ Converted customer_age to int32\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "# Data Types after changing\n",
        "print(\"\\nCurrent data types:\")\n",
        "print(df.dtypes)\n"
      ],
      "metadata": {
        "id": "IbJShR15ymBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: DateTime Parsing"
      ],
      "metadata": {
        "id": "Ikn56SSH2Aac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DateTime Parsing :\n",
        "# converting a string of text that represents a date and/or time into a structured, machine-readable format\n",
        "print(\"PARSING DATE COLUMNS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "date_columns = ['date_of_purchase', 'first_response_time', 'time_to_resolution']\n",
        "for col in date_columns:\n",
        "    if col in df.columns and df[col].dtype == 'object':\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "        print(f\"✓ Converted '{col}' to datetime\")"
      ],
      "metadata": {
        "id": "ezqllZmQ1p51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "FHQAeUXk4C8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: CREATING NEW FEATURES"
      ],
      "metadata": {
        "id": "WXBjC3x-33-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CREATING NEW FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'date_of_purchase' in df.columns:\n",
        "    df['purchase_year'] = df['date_of_purchase'].dt.year\n",
        "    df['purchase_month'] = df['date_of_purchase'].dt.month\n",
        "    df['purchase_day'] = df['date_of_purchase'].dt.day\n",
        "    print(f\"✓ Extracted year, month, day from date_of_purchase\")"
      ],
      "metadata": {
        "id": "iP6TtkxE3uIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "oil1IxJo36kQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: Encoding Categorical Data"
      ],
      "metadata": {
        "id": "ADFiWImT5Si_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ENCODING CATEGORICAL COLUMNS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "encoders = {}\n",
        "\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    df[col + '_encoded'] = le.fit_transform(df[col].astype(str))\n",
        "    encoders[col] = le\n",
        "    print(f\"✓ Encoded '{col}'\")"
      ],
      "metadata": {
        "id": "fJhu2wAS4U_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "03P-ZUov5E7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Normalization"
      ],
      "metadata": {
        "id": "_NnO3TUd5wPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"NORMALIZING NUMERIC COLUMNS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'customer_age' in df.columns:\n",
        "    scaler = MinMaxScaler()\n",
        "    df['customer_age_normalized'] = scaler.fit_transform(df[['customer_age']])\n",
        "    print(f\"✓ Normalized customer_age to 0-1 range\")\n",
        "    print(f\"  Range: [{df['customer_age_normalized'].min():.2f}, {df['customer_age_normalized'].max():.2f}]\")\n"
      ],
      "metadata": {
        "id": "Ojklzd8s5tX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "bFgl7Nf25-bY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Saving the Transformed Data after applying the Data Transformation Method"
      ],
      "metadata": {
        "id": "OCLiJyjY6XQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concluded:\n",
        "print(\"TRANSFORMATION COMPLETE\")\n",
        "print(f\"Final size: {len(df)} rows × {len(df.columns)} columns\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('03_transformed_data.csv', index=False)\n",
        "print(\"✓ Saved: 03_transformed_data.csv\")\n"
      ],
      "metadata": {
        "id": "KCwbCuDZx54i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### METHOD 4: DATA FILTERING"
      ],
      "metadata": {
        "id": "ehqKtoop7sZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Remove Unnecessary Columns"
      ],
      "metadata": {
        "id": "em_uh3p37v80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload Load Transformed data\n",
        "df = pd.read_csv('03_transformed_data.csv')\n",
        "\n",
        "# Check Data Types in the validated data sheet\n",
        "print(\"\\nCurrent data types:\")\n",
        "print(df.dtypes)\n",
        "print(\"=\" * 80)\n",
        "\n",
        "#Remove Unnecessary Columns\n",
        "print(\"STEP 1: REMOVING UNNECESSARY COLUMNS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "columns_to_remove = ['customer_name', 'customer_email']\n",
        "for col in columns_to_remove:\n",
        "    if col in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "        print(f\"✓ Removed: '{col}'\")\n",
        "\n",
        "# Remove original text columns that we encoded\n",
        "text_cols = df.select_dtypes(include=['object']).columns\n",
        "for col in text_cols:\n",
        "    if col + '_encoded' in df.columns:\n",
        "        df = df.drop(columns=[col])\n",
        "        print(f\"✓ Removed: '{col}' (using encoded version)\")"
      ],
      "metadata": {
        "id": "V6bL8H4o8Skn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "7jINfXcc9HGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Filter Records by Condition"
      ],
      "metadata": {
        "id": "G_xsoLhV-TBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter Records by applying Condition\n",
        "print(\"FILTERING RECORDS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'customer_satisfaction_rating' in df.columns:\n",
        "    df = df.dropna(subset=['customer_satisfaction_rating'])\n",
        "    print(f\"✓ Filtered out unresolved tickets\")\n",
        "\n",
        "if 'purchase_year' in df.columns:\n",
        "    recent_year = df['purchase_year'].max() - 2\n",
        "    df = df[df['purchase_year'] >= recent_year]\n",
        "    print(f\"✓ Kept only recent data (year >= {recent_year})\")"
      ],
      "metadata": {
        "id": "AZPt7RIZ-P9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "WwEHKL8D-vIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 3: Remove Incomplete Records"
      ],
      "metadata": {
        "id": "_JNqg0wG_Zrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Incomplete Records form the datasets\n",
        "print(\"FILTERING INCOMPLETE RECORDS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "initial = len(df)\n",
        "critical_fields = ['customer_age', 'ticket_status', 'ticket_priority']\n",
        "df = df.dropna(subset=[col for col in critical_fields if col in df.columns])\n",
        "print(f\"✓ Removed {initial - len(df)} rows with missing critical fields\")\n",
        "\n",
        "print(f\"\\nAfter: {len(df)} rows × {len(df.columns)} columns\")"
      ],
      "metadata": {
        "id": "QNSMki7LATNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Saving the Filtered Data after applying the Data Filtration Method"
      ],
      "metadata": {
        "id": "NN3WpX7xBKrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concluded:\n",
        "print(\"DATA FILTERING COMPLETE\")\n",
        "print(f\"Final size: {len(df)} rows × {len(df.columns)} columns\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('04_filtered_data.csv', index=False)\n",
        "print(\"✓ Saved: 04_filtered_data.csv\")\n"
      ],
      "metadata": {
        "id": "tPa3ci0HA0EK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###METHOD 5: DATA ENRICHMENT"
      ],
      "metadata": {
        "id": "iY1bJejgBaE-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 1: Create Derived Fields"
      ],
      "metadata": {
        "id": "CR8CLQb3BvAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload Load Filtered data\n",
        "df = pd.read_csv('04_filtered_data.csv')\n",
        "\n",
        "# Create Derived Fields\n",
        "print(\"CREATING DERIVED FIELDS:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def categorize_age(age):\n",
        "    \"\"\"Categorize age into groups\"\"\"\n",
        "    if age < 30:\n",
        "        return 'Young'\n",
        "    elif age < 50:\n",
        "        return 'Middle-aged'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "if 'customer_age' in df.columns:\n",
        "    df['age_group'] = df['customer_age'].apply(categorize_age)\n",
        "    print(\"✓ Created age_group field\")\n",
        "    print(f\"  Categories: {df['age_group'].unique()}\")"
      ],
      "metadata": {
        "id": "UsgvOcV9BkAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "IYgibRB7Cduc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visual Representation of the data\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Set the visual style\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# 2. Define the desired order for the categories\n",
        "age_order = ['Young', 'Middle-aged', 'Senior']\n",
        "\n",
        "# 3. Create the countplot\n",
        "ax = sns.countplot(\n",
        "    data=df,\n",
        "    x='age_group',\n",
        "    order=age_order,\n",
        "    palette='viridis'\n",
        ")\n",
        "\n",
        "# 4. Add labels and title for clarity\n",
        "plt.title('Distribution of Customers by Age Group', fontsize=15, pad=20)\n",
        "plt.xlabel('Age Group', fontsize=12)\n",
        "plt.ylabel('Number of Customers', fontsize=12)\n",
        "\n",
        "# 5. Add data labels on top of each bar (optional but helpful)\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                xytext=(0, 9),\n",
        "                textcoords='offset points')\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "In8v3WpQDm5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 2: Lookup-Based Enrichment"
      ],
      "metadata": {
        "id": "2pGUvVxuDA7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ADDING REFERENCE DATA:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "priority_mapping = {\n",
        "    'low': 1,\n",
        "    'medium': 2,\n",
        "    'high': 3,\n",
        "    'critical': 4\n",
        "}\n",
        "\n",
        "print(\"✓ Created priority mapping reference\")"
      ],
      "metadata": {
        "id": "JgIMSv7tC0cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 4: Interaction Features"
      ],
      "metadata": {
        "id": "T79wnQZJF6VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"CREATING INTERACTION FEATURES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'customer_age' in df.columns and 'customer_satisfaction_rating' in df.columns:\n",
        "    df['customer_value_score'] = (df['customer_age'] * df['customer_satisfaction_rating']) / 100\n",
        "    print(\"✓ Created customer_value_score\")\n"
      ],
      "metadata": {
        "id": "GZqWJomcF4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 5: Time-Based Features"
      ],
      "metadata": {
        "id": "muHMT3YDGDRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"EXTRACTING TIME FEATURES:\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if 'purchase_month' in df.columns:\n",
        "    def get_season(month):\n",
        "        if month in [12, 1, 2]:\n",
        "            return 'Winter'\n",
        "        elif month in [3, 4, 5]:\n",
        "            return 'Spring'\n",
        "        elif month in [6, 7, 8]:\n",
        "            return 'Summer'\n",
        "        else:\n",
        "            return 'Fall'\n",
        "\n",
        "    df['season'] = df['purchase_month'].apply(get_season)\n",
        "    print(\"✓ Created season field\")"
      ],
      "metadata": {
        "id": "fRSweSSoGCk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "KL08WCkeGYxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Saving the Enriched Data after applying the Data Enrichment Method"
      ],
      "metadata": {
        "id": "eRrUASQCGvwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Concluded:\n",
        "print(\"DATA ENRICHMENT COMPLETE\")\n",
        "print(f\"Final size: {len(df)} rows × {len(df.columns)} columns\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save to CSV in Google Drive\n",
        "df.to_csv('05_enriched_data.csv', index=False)\n",
        "print(\"✓ Saved: 05_enriched_data.csv\")"
      ],
      "metadata": {
        "id": "OuTQL5xlBWKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "uTLlgXKdIrav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}